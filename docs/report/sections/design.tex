\section{Design}

\subsection{Architecture}

\subsection{User Interface}

\subsection{Database}

\subsubsection{PostgreSQL}

PostgreSQL was used for this project because of its high scability and stability.
Complex queries are performed faster in PostgreSQL than alternatives such as MySQL
which is important for optimising our search times especially for a large property
dataset. PostgreSQL is also ACID (Atomicity, Consistency, Isolation, Durability)
compliant, which ensures that no data is lost in the system in the case of failures.
Since we receive a large number of database read and writes towards the end
of an auction session, concurrency control is vital to ensure the consistency
of data. PostgreSQL efficiently handles concurrency with its MVCC (Multiversion
Concurrency Control). Each database query sees only a snapshot of the database
when it was accessed, preventing the query from seeing inconsistent data that
could be caused by other concurrent updates on the same data. PostgreSQL is also
horizontally scalable as it supports data replication and sharding.
This is important if our project was to be extended and deployed outside the NSW
region and potentially globally.

\subsubsection{Database of localhost}

A full entity relationship diagram of our database can be found in Appendix.
The database of \emph{localhost} is normalised in 3NF (third normal form) as it
contains no transitive dependencies, minimising our data redundancy and
improving data integrity. Extensibility was kept in mind when designing the
database schema.

\subsubsection{Search Time Optimisation}

Property locations are determined by their latitude and longitude stored in the
database. When a user searches a location, the Google Maps API converts the location
into a latitude and longitude which is then used as the target destination.
The results of our property search are shown in order of closest distance to the
target destination.
Distance calculation is performed using Haversine's formula, which determines the
great-circle distance between two points in NSW. Vincenty's formula is a popular and
more accurate alternative for calculating distance, however we chose to
use Haversine's formula as it is computationally much faster.

\begin{lstlisting}[caption={Haversine's Formula}]
radlat = Radians(latitude)                      # Destination Lat
radlong = Radians(longitude)                    # Destination Long
radflat = Radians(models.F('latitude'))	        # Property Lat
radflong = Radians(models.F('longitude'))       # Property Long

distance = 6371 * Acos(
    Cos(radlat) * Cos(radflat) * Cos(radflong - radlong) +
    Sin(radlat) * Sin(radflat))
\end{lstlisting}

Our search query calculates the distance from each property to the target
destination using Haversine's Formula and then orders them by closest distance.
Since it is costly to calculate this for every property in the database, an
initial filter was added to narrow the number of properties that the
distance calculation had to be operated on.

\begin{lstlisting}[caption={Initial filter to narrow search results}]
lat_offset, lng_offset = Decimal(0.15), Decimal(0.15)
lat_range = (lat - lat_offset, lat + lat_offset)
lng_range = (lng - lng_offset, lng + lng_offset)
properties = Property.objects.within(lat, lng).filter(
    latitude__range=lat_range, longitude__range=lng_range)
\end{lstlisting}

Only properties within $\pm$0.15 latitude and longitude offsets of the original
search, which translates to around 15km, are shown. This greatly improves the
search times as well as removes properties that were outside a reasonable distance
from the search. By adding the initial filter, our search time is also no longer
restricted by the property dataset and is instead limited by suburb density.

\subsection{Event Scheduling}

\subsection{Real-Time Communication}
